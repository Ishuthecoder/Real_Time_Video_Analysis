import cv2
import face_recognition
import numpy as np
from keras.preprocessing import image
from keras.models import model_from_json,load_model

# Get the video capture
video_stream=cv2.VideoCapture(0)


face_exp_model=load_model(r"C:\Users\dubey\Downloads\facial_expression_model_combined.h5")

print("Model saved as 'combined_emotion_model.h5'")

# list of emotions on face
emotions_label = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')

# Initialize empty array for face locations
all_face_locations = []

# Create a while loop to loop through each frame of video
while True:
    ret, current_frame = video_stream.read()

    # Break the loop if there are no frames
    if not ret:
        break

    # Resize the current frame to process faster
    current_frame_small = cv2.resize(current_frame, (0, 0), fx=0.25, fy=0.25)

    # Detect all faces in the video
    all_face_locations = face_recognition.face_locations(current_frame_small, number_of_times_to_upsample=2, model='hog')

    # Loop through face locations
    for top, right, bottom, left in all_face_locations:
        # Change the position magnitude to fit the actual size of the video frame
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        # Print the position
        print('Found face at locations Top:{}, Left:{}, Bottom:{}, Right:{}'.format(top, left, bottom, right))

        # Draw a rectangle on the face
        cv2.rectangle(current_frame, (left, top), (right, bottom), (0, 0, 255), 2)

        # Extract the face from the current frame
        current_face_image = current_frame[top:bottom, left:right]

        # Preprocess input: convert to grayscale
        current_face_image = cv2.cvtColor(current_face_image, cv2.COLOR_BGR2GRAY)

        # Resize to 48x48 pixels (as required by the model)
        current_face_image = cv2.resize(current_face_image, (48, 48))

        # Convert the image into a 3D numpy array
        img_pixels = image.img_to_array(current_face_image)

        # Expand the shape of array into single multiple columns
        img_pixels = np.expand_dims(img_pixels, axis=0)

        # Prediction using the model, get prediction values for all 7 emotions
        exp_predictions = face_exp_model.predict(img_pixels)

        # Find the index of the maximum prediction value (0 to 6)
        max_index = np.argmax(exp_predictions[0])

        # Get corresponding label from emotions_label
        emotion_label = emotions_label[max_index]

        # Display the emotion label as text on the image
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(current_frame, emotion_label, (left, bottom), font, 0.5, (255, 255, 255), 1)

    # Show the current frame with face detection and emotion recognition
    cv2.imshow('Face Detection', current_frame)

    # Press 'q' on the keyboard to break the while loop
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the stream and camera
# Close all open OpenCV windows
video_stream.release()
cv2.destroyAllWindows()
